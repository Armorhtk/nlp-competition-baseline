{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 非标准化诊疗强基线Baseline-1.65+冲进前二十\n",
    "\n",
    "本Baseline基于[阿水大佬](https://github.com/datawhalechina/competition-baseline/blob/master/competition/%E7%A7%91%E5%A4%A7%E8%AE%AF%E9%A3%9EAI%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E8%B5%9B2022/%E9%9D%9E%E6%A0%87%E5%87%86%E5%8C%96%E7%96%BE%E7%97%85%E8%AF%89%E6%B1%82%E7%9A%84%E7%AE%80%E5%8D%95%E5%88%86%E8%AF%8A%E6%8C%91%E6%88%98%E8%B5%9B_baseline.ipynb)开源的baseline上进行优化。\n",
    "\n",
    "# 改进内容\n",
    "\n",
    "1. 更换医疗预训练模型MC-BERT：相比使用通用预训练模型，直接使用医疗预训练模型能够触类而长得到更强的基线。↑ [MC-BERT学习资料](https://mp.weixin.qq.com/s?__biz=MzI0NTU5OTM5Nw==&mid=2247483665&idx=1&sn=616596615e201191c043b0037586516b&chksm=e94d5cdbde3ad5cd40dfefb8ec9958d40e07a639976d852e118f6fc36e56bfc842bb720d9a49&token=1618266258&lang=zh_CN#rd)\n",
    "\n",
    "2. 灵活设计字段拼接方式：使用空格or逗号拼接字段的优点是简单，但句子并不连贯。灵活设计字段拼接，人工增加前缀使得句子语义连贯，在使用[sep]、[unseed]等特殊作用的标志位进行拼接（去年Top3均采用了此方式）会有明显的提升。↑ [点击观看2021年Top3答辩视频](https://www.bilibili.com/video/BV1hq4y1r7xB?p=15&vd_source=4cb6c45950d255204ceab92e062fd928)\n",
    "\n",
    "3. CLS or avgpooling ：平均池化BERT模型最后一层的特征,相较于直接使用CLS位置的特征可能效果更好，（部分比赛ticks分享中，融合BERT后4层的向量也可能提升下游任务性能）。↑\n",
    "\n",
    "4. 遵循赛题要求使用marco-f1和mirco-f1评测模型：使用Accuracy评测模型效果线下效果和线上效果差异过大。↑（线下1.68，线上1.65）\n",
    "\n",
    "# 后续优化\n",
    "\n",
    "- 对抗训练FGM、PGM 学习资料[对抗训练](https://github.com/Zeta-THU/NLP-Interview-Notes/blob/9a978660bbb2009956a49067437ec945c10abed8/Trick/SmallSampleProblem/AdversarialTraining/AdversarialTraining.md)\n",
    "\n",
    "- 不平衡类别损失函数 focal_loss、Dice_loss、DSC Loss 学习资料[NLP-Loss-Pytorch](https://github.com/shuxinyin/NLP-Loss-Pytorch)\n",
    "\n",
    "- RDrop对比学习 学习资料[R-Drop：提升有监督任务性能最简单的方法](https://zhuanlan.zhihu.com/p/418305402)\n",
    "\n",
    "- 设计新的损失函数，考虑label_i和label_j之间的层次关系\n",
    "\n",
    "- 融合主体/属性/客体等三元组知识"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22868, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_excel('非标准化疾病诉求的简单分诊挑战赛2/data_train.xlsx')\n",
    "test_df = pd.read_excel('非标准化疾病诉求的简单分诊挑战赛2/data_test.xlsx')\n",
    "test_submit = pd.read_csv('data/提交示例.csv')\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 原始 : 简单拼接\n",
    "train_text = train_df['diseaseName'] + ' ' + train_df['conditionDesc'] + ' ' + train_df['title'] + ' ' + train_df['hopeHelp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "简单拼接:\n",
      "右膝前交叉韧带断裂，半月板三度重度损伤 右膝受力小腿骨和大腿骨就会滑开，走路会有隐隐作痛 大概做韧带重建手术需要多少费用？ 大概做韧带重建手术需要多少费用？\n",
      "优化拼接：\n",
      "疾病名称:右膝前交叉韧带断裂，半月板三度重度损伤[unused01]症状描述:右膝受力小腿骨和大腿骨就会滑开，走路会有隐隐作痛[unused01]大概做韧带重建手术需要多少费用？\n"
     ]
    }
   ],
   "source": [
    "# 优化拼接方法 : 清洗噪声符号 + 人工构建字段描述模板 + 去重重复字段 + 使用vocab.txt中的[unused01]连接各字段\n",
    "def clean_str(x):\n",
    "    return x.replace('\\r', '').replace('\\t', ' ').replace('\\n', ' ')\n",
    "\n",
    "merge_col = [\"diseaseName\",\"conditionDesc\",\"title\",\"hopeHelp\"]\n",
    "for col in merge_col:\n",
    "    train_df[col].fillna('', inplace=True)\n",
    "    test_df[col].fillna('', inplace=True)\n",
    "    train_df[col] = train_df[col].apply(lambda x: clean_str(x))\n",
    "    test_df[col] = test_df[col].apply(lambda x: clean_str(x))\n",
    "\n",
    "sentences = []\n",
    "for idx,rows in train_df.iterrows():\n",
    "    merge_col_template = {\"diseaseName\":\"疾病名称:\",\"conditionDesc\":\"症状描述:\",\"title\":\"\",\"hopeHelp\":\"\"}\n",
    "    if rows[\"title\"] in rows[\"conditionDesc\"] or rows[\"title\"] in rows[\"diseaseName\"] or rows[\"title\"] == rows[\"hopeHelp\"] or len(rows[\"title\"]) == 0:\n",
    "        merge_col_template.pop(\"title\")\n",
    "    if  len(rows[\"hopeHelp\"]) == 0:\n",
    "        merge_col_template.pop(\"hopeHelp\")\n",
    "    sentence = \"[unused01]\".join([str(temp) + str(rows[col]) for col,temp in merge_col_template.items()])\n",
    "    sentences.append(sentence)\n",
    "train_df[\"sentence\"] = sentences\n",
    "\n",
    "sentences = []\n",
    "for idx,rows in test_df.iterrows():\n",
    "    merge_col_template = {\"diseaseName\":\"疾病名称:\",\"conditionDesc\":\"症状描述:\",\"title\":\"\",\"hopeHelp\":\"\"}\n",
    "    if rows[\"title\"] in rows[\"conditionDesc\"] or rows[\"title\"] in rows[\"diseaseName\"] or rows[\"title\"] == rows[\"hopeHelp\"] or len(rows[\"title\"]) == 0:\n",
    "        merge_col_template.pop(\"title\")\n",
    "    if  len(rows[\"hopeHelp\"]) == 0:\n",
    "        merge_col_template.pop(\"hopeHelp\")\n",
    "    sentence = \"[unused01]\".join([str(temp) + str(rows[col]) for col,temp in merge_col_template.items()])\n",
    "    sentences.append(sentence)\n",
    "test_df[\"sentence\"] = sentences\n",
    "\n",
    "# 简单比较一下拼接后的句子\n",
    "print('简单拼接:')\n",
    "print(train_text[1])\n",
    "print('优化拼接：')\n",
    "print(train_df['sentence'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8746, 9)\n",
      "(12709, 9)\n",
      "(21455, 9)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertModel,BertTokenizer\n",
    "pretrain_dir = r'PretrainModel/mc_bert_base'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrain_dir,additional_special_tokens=['[unused99]'])\n",
    "# 处理label j 无标记数据，拼接在乱序数据集后面\n",
    "unlabeled_train_df = train_df[train_df['label_j']==-1]\n",
    "print(unlabeled_train_df.shape)\n",
    "train_df = train_df[train_df['label_j']!=-1]\n",
    "# 划分数据集\n",
    "train_text,val_text,_,_ = train_test_split(train_df,train_df['id'],test_size=0.1,random_state=42)\n",
    "print(train_text.shape)\n",
    "train_text = pd.concat([train_text.sample(frac=1.0,random_state=42),unlabeled_train_df],axis=0)\n",
    "print(train_text.shape)\n",
    "train_encoding = tokenizer(train_text['sentence'].tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "val_encoding = tokenizer(val_text['sentence'].tolist(), truncation=True, padding='max_length', max_length=256)\n",
    "test_encoding = tokenizer(test_df['sentence'].tolist(), truncation=True, padding='max_length', max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 疾 病 描 述 : 右 膝 前 交 叉 韧 带 断 裂 [unused99] 症 状 描 述 : 右 膝 受 力 小 腿 骨 和 大 腿 骨 就 会 滑 开 [unused99] 大 概 做 韧 带 重 建 手 术 需 要 多 少 费 用 ？ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看编码\n",
    "tokenizer.decode(tokenizer(\"疾病描述:右膝前交叉韧带断裂[unused99]症状描述:右膝受力小腿骨和大腿骨就会滑开[unused99]大概做韧带重建手术需要多少费用？\",truncation=True, padding='max_length', max_length=256)[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch import nn\n",
    "# 数据集读取\n",
    "class XunFeiDataset(Dataset):\n",
    "    def __init__(self, encodings, label_i, label_j):\n",
    "        self.encodings = encodings\n",
    "        self.label_i = label_i\n",
    "        self.label_j = label_j\n",
    "    # 读取单个样本\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['label_i'] = torch.tensor(int(self.label_i[idx]))\n",
    "        item['label_j'] = torch.tensor(int(self.label_j[idx]))\n",
    "        return item\n",
    "    def __len__(self):\n",
    "        return len(self.label_i)\n",
    "\n",
    "train_dataset = XunFeiDataset(train_encoding,\n",
    "                              train_text['label_i'].values,\n",
    "                              train_text['label_j'].values)\n",
    "val_dataset = XunFeiDataset(val_encoding,\n",
    "                            val_text['label_i'].values,\n",
    "                            val_text['label_j'].values)\n",
    "test_dataset = XunFeiDataset(test_encoding,\n",
    "                             [0] * len(test_df),\n",
    "                             [0] * len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 单个读取到批量读取\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class XunFeiModel(nn.Module):\n",
    "    def __init__(self, num_labels_i, num_labels_j):\n",
    "        super(XunFeiModel,self).__init__()\n",
    "        #Load Model with given checkpoint and extract its body\n",
    "        self.model = BertModel.from_pretrained(pretrain_dir)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier_i = nn.Linear(768, num_labels_i)\n",
    "        self.classifier_j = nn.Linear(768, num_labels_j)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None,labels=None):\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # outputs是一个包含两个输出的元组\n",
    "        # outputs[0] = last hidden state 输出形状:(batch_size,max_len,768)\n",
    "        # outputs[1] = pooing hidden state 输出形状:(batch_size,768)\n",
    "        sequence_output = self.dropout(outputs[1])\n",
    "        # 阿水大佬原版 sequence_output[:,0,:].view(-1,768) 相当于取CLS的句向量 输出形状：(batch_size,768)\n",
    "        logits_i = self.classifier_i(sequence_output) # 输出形状：(batch_size,num_labels_i)\n",
    "        logits_j = self.classifier_j(sequence_output) # 输出形状：(batch_size,num_labels_j)\n",
    "\n",
    "        return logits_i, logits_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at E:\\HuProject\\NLPsource\\PretrainModel\\mc_bert_base were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class_i_nums = len(train_text['label_i'].unique()) # 20\n",
    "class_j_nums = len(train_text['label_j'].unique()) - 1  # 61\n",
    "model = XunFeiModel(class_i_nums, class_j_nums)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = 'cpu' or 'gpu'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AdamW,get_linear_schedule_with_warmup\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", 'LayerNorm.bias', \"LayerNorm.weight\", 'layer_norm.bias', 'layer_norm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0\n",
    "    },\n",
    "]\n",
    "# 设置学习率\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(epochs=10):\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0.0, num_training_steps=len(train_dataloader) * epochs)\n",
    "    print(\"train ...\")\n",
    "    total_iter_num = 0\n",
    "    best_score = 0.0\n",
    "    total_iter = len(train_dataloader) * epochs\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        iter_num = 0\n",
    "        total_train_loss = 0\n",
    "        pred_is,pred_js = [],[]\n",
    "        label_is,label_js = [],[]\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            label_i = batch['label_i'].to(device)\n",
    "            label_j = batch['label_j'].to(device)\n",
    "            pred_i, pred_j = model(input_ids,attention_mask)\n",
    "\n",
    "            # 疾病方向标签-1时，不记录损失\n",
    "            valid = label_j != -1\n",
    "            if len(pred_j[valid]) != 0:\n",
    "                loss = loss_fn(pred_i, label_i)  + loss_fn(pred_j[valid], label_j[valid])\n",
    "            else:\n",
    "                loss = loss_fn(pred_i, label_i)\n",
    "            # 反向梯度信息\n",
    "            loss.backward()\n",
    "            # 参数更新\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            iter_num += 1\n",
    "            total_iter_num += 1\n",
    "\n",
    "            pred_is.extend(pred_i.argmax(1).data.cpu().numpy())\n",
    "            label_is.extend(label_i.data.cpu().numpy())\n",
    "\n",
    "            if len(pred_j[valid]) != 0:\n",
    "                pred_js.extend(pred_j[valid].argmax(1).data.cpu().numpy())\n",
    "                label_js.extend(label_j[valid].data.cpu().numpy())\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            if(total_iter_num % 50 == 0):\n",
    "                lis = f1_score(y_true=pred_is,y_pred=label_is,average='macro')\n",
    "                ljs = f1_score(y_true=pred_js,y_pred=label_js,average='micro')\n",
    "                print(\"iter_num: %d, step:%.2f%% , loss: %.4f,  label_i:%.4f label_j:%.4f score:%.4f\" % (\n",
    "                    total_iter_num , total_iter_num  / total_iter, total_train_loss/iter_num ,\n",
    "                    lis,\n",
    "                    ljs,\n",
    "                    (lis + ljs)\n",
    "                ))\n",
    "            # 验证\n",
    "            if(total_iter_num % 200 == 0):\n",
    "                _,_,score = validation()\n",
    "                if best_score < score:\n",
    "                    print(\"score: %.4f -----> %.4f\" % (best_score,score))\n",
    "                    best_score = score\n",
    "                    print(\"保存模型\")\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "                    # output_model_file = 'model_{:.4f}_{}.bin'.format(best_score,str(iter_num))\n",
    "                    output_model_file = 'model_best.bin'\n",
    "                    torch.save(model_to_save, output_model_file)\n",
    "\n",
    "    return best_score\n",
    "\n",
    "def validation():\n",
    "    model.eval()\n",
    "    pred_is,pred_js = [],[]\n",
    "    label_is,label_js = [],[]\n",
    "    for batch in val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            label_i = batch['label_i'].to(device)\n",
    "            label_j = batch['label_j'].to(device)\n",
    "\n",
    "            pred_i, pred_j = model(\n",
    "                input_ids,\n",
    "                attention_mask\n",
    "            )\n",
    "            pred_is.extend(pred_i.argmax(1).data.cpu().numpy())\n",
    "            pred_js.extend(pred_j.argmax(1).data.cpu().numpy())\n",
    "            label_is.extend(label_i.data.cpu().numpy())\n",
    "            label_js.extend(label_j.data.cpu().numpy())\n",
    "\n",
    "    lis = f1_score(y_true=pred_is,y_pred=label_is,average='macro')\n",
    "    ljs = f1_score(y_true=pred_js,y_pred=label_js,average='micro')\n",
    "\n",
    "    print(\"-------------------------------\")\n",
    "    print(\"label_i:%.4f label_j:%.4f score:%.4f\" % (lis,ljs,(lis + ljs)))\n",
    "    print(\"-------------------------------\")\n",
    "    model.train()\n",
    "    return lis,ljs,(lis + ljs)\n",
    "\n",
    "def prediction():\n",
    "    model.eval()\n",
    "    test_label_i = []\n",
    "    test_label_j = []\n",
    "    for batch in tqdm(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            pred_i, pred_j = model(input_ids, attention_mask)\n",
    "            test_label_i += list(pred_i.argmax(1).data.cpu().numpy())\n",
    "            test_label_j += list(pred_j.argmax(1).data.cpu().numpy())\n",
    "    return test_label_i, test_label_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train ...\n",
      "iter_num: 50, step:0.00% , loss: 6.5798,  label_i:0.2255 label_j:0.0925 score:0.3180\n",
      "iter_num: 100, step:0.01% , loss: 5.8392,  label_i:0.4513 label_j:0.1931 score:0.6444\n",
      "iter_num: 150, step:0.01% , loss: 5.2768,  label_i:0.5456 label_j:0.2692 score:0.8148\n",
      "iter_num: 200, step:0.01% , loss: 4.8503,  label_i:0.6138 label_j:0.3162 score:0.9300\n",
      "-------------------------------\n",
      "label_i:0.7869 label_j:0.5025 score:1.2894\n",
      "-------------------------------\n",
      "score: 0.0000 -----> 1.2894\n",
      "保存模型\n",
      "iter_num: 250, step:0.02% , loss: 4.5259,  label_i:0.6555 label_j:0.3560 score:1.0115\n",
      "iter_num: 300, step:0.02% , loss: 4.2683,  label_i:0.6843 label_j:0.3794 score:1.0637\n",
      "iter_num: 350, step:0.03% , loss: 4.0560,  label_i:0.7031 label_j:0.4007 score:1.1038\n",
      "iter_num: 400, step:0.03% , loss: 3.8863,  label_i:0.7172 label_j:0.4142 score:1.1314\n",
      "-------------------------------\n",
      "label_i:0.8213 label_j:0.5414 score:1.3627\n",
      "-------------------------------\n",
      "score: 1.2894 -----> 1.3627\n",
      "保存模型\n",
      "iter_num: 450, step:0.03% , loss: 3.7253,  label_i:0.7287 label_j:0.4299 score:1.1586\n",
      "iter_num: 500, step:0.04% , loss: 3.6015,  label_i:0.7368 label_j:0.4414 score:1.1781\n",
      "iter_num: 550, step:0.04% , loss: 3.4842,  label_i:0.7443 label_j:0.4537 score:1.1981\n",
      "iter_num: 600, step:0.04% , loss: 3.3761,  label_i:0.7518 label_j:0.4656 score:1.2175\n",
      "-------------------------------\n",
      "label_i:0.8425 label_j:0.6037 score:1.4461\n",
      "-------------------------------\n",
      "score: 1.3627 -----> 1.4461\n",
      "保存模型\n",
      "iter_num: 650, step:0.05% , loss: 3.2902,  label_i:0.7552 label_j:0.4759 score:1.2311\n",
      "iter_num: 700, step:0.05% , loss: 3.2042,  label_i:0.7604 label_j:0.4853 score:1.2456\n",
      "iter_num: 750, step:0.06% , loss: 3.1235,  label_i:0.7649 label_j:0.4943 score:1.2591\n",
      "iter_num: 800, step:0.06% , loss: 3.0306,  label_i:0.7706 label_j:0.5039 score:1.2745\n",
      "-------------------------------\n",
      "label_i:0.8617 label_j:0.6221 score:1.4837\n",
      "-------------------------------\n",
      "score: 1.4461 -----> 1.4837\n",
      "保存模型\n",
      "iter_num: 850, step:0.06% , loss: 2.8875,  label_i:0.7761 label_j:0.5039 score:1.2800\n",
      "iter_num: 900, step:0.07% , loss: 2.7521,  label_i:0.7818 label_j:0.5039 score:1.2857\n",
      "iter_num: 950, step:0.07% , loss: 2.6279,  label_i:0.7878 label_j:0.5039 score:1.2917\n",
      "iter_num: 1000, step:0.07% , loss: 2.5164,  label_i:0.7930 label_j:0.5039 score:1.2969\n",
      "-------------------------------\n",
      "label_i:0.8233 label_j:0.6285 score:1.4518\n",
      "-------------------------------\n",
      "iter_num: 1050, step:0.08% , loss: 2.4166,  label_i:0.7970 label_j:0.5039 score:1.3009\n",
      "iter_num: 1100, step:0.08% , loss: 2.3221,  label_i:0.8021 label_j:0.5039 score:1.3060\n",
      "iter_num: 1150, step:0.09% , loss: 2.2358,  label_i:0.8059 label_j:0.5039 score:1.3098\n",
      "iter_num: 1200, step:0.09% , loss: 2.1546,  label_i:0.8101 label_j:0.5039 score:1.3140\n",
      "-------------------------------\n",
      "label_i:0.8219 label_j:0.6256 score:1.4475\n",
      "-------------------------------\n",
      "iter_num: 1250, step:0.09% , loss: 2.0820,  label_i:0.8132 label_j:0.5039 score:1.3171\n",
      "iter_num: 1300, step:0.10% , loss: 2.0144,  label_i:0.8162 label_j:0.5039 score:1.3201\n",
      "iter_num: 1350, step:0.10% , loss: 1.9212,  label_i:0.7903 label_j:0.6389 score:1.4291\n",
      "iter_num: 1400, step:0.10% , loss: 1.9265,  label_i:0.8365 label_j:0.6123 score:1.4488\n",
      "-------------------------------\n",
      "label_i:0.8530 label_j:0.6355 score:1.4885\n",
      "-------------------------------\n",
      "score: 1.4837 -----> 1.4885\n",
      "保存模型\n",
      "iter_num: 1450, step:0.11% , loss: 1.8293,  label_i:0.8482 label_j:0.6313 score:1.4795\n",
      "iter_num: 1500, step:0.11% , loss: 1.7693,  label_i:0.8557 label_j:0.6427 score:1.4983\n",
      "iter_num: 1550, step:0.12% , loss: 1.7135,  label_i:0.8616 label_j:0.6489 score:1.5106\n",
      "iter_num: 1600, step:0.12% , loss: 1.6767,  label_i:0.8636 label_j:0.6566 score:1.5202\n",
      "-------------------------------\n",
      "label_i:0.8527 label_j:0.6391 score:1.4918\n",
      "-------------------------------\n",
      "score: 1.4885 -----> 1.4918\n",
      "保存模型\n",
      "iter_num: 1650, step:0.12% , loss: 1.6522,  label_i:0.8647 label_j:0.6588 score:1.5235\n",
      "iter_num: 1700, step:0.13% , loss: 1.6338,  label_i:0.8663 label_j:0.6602 score:1.5264\n",
      "iter_num: 1750, step:0.13% , loss: 1.6237,  label_i:0.8666 label_j:0.6598 score:1.5264\n",
      "iter_num: 1800, step:0.13% , loss: 1.5997,  label_i:0.8677 label_j:0.6638 score:1.5315\n",
      "-------------------------------\n",
      "label_i:0.8597 label_j:0.6737 score:1.5335\n",
      "-------------------------------\n",
      "score: 1.4918 -----> 1.5335\n",
      "保存模型\n",
      "iter_num: 1850, step:0.14% , loss: 1.5859,  label_i:0.8680 label_j:0.6656 score:1.5337\n",
      "iter_num: 1900, step:0.14% , loss: 1.5716,  label_i:0.8683 label_j:0.6645 score:1.5327\n",
      "iter_num: 1950, step:0.15% , loss: 1.5506,  label_i:0.8712 label_j:0.6695 score:1.5407\n",
      "iter_num: 2000, step:0.15% , loss: 1.5408,  label_i:0.8711 label_j:0.6712 score:1.5423\n",
      "-------------------------------\n",
      "label_i:0.8574 label_j:0.6886 score:1.5460\n",
      "-------------------------------\n",
      "score: 1.5335 -----> 1.5460\n",
      "保存模型\n",
      "iter_num: 2050, step:0.15% , loss: 1.5243,  label_i:0.8725 label_j:0.6734 score:1.5459\n",
      "iter_num: 2100, step:0.16% , loss: 1.5053,  label_i:0.8735 label_j:0.6765 score:1.5500\n",
      "iter_num: 2150, step:0.16% , loss: 1.4665,  label_i:0.8749 label_j:0.6799 score:1.5548\n",
      "iter_num: 2200, step:0.16% , loss: 1.3994,  label_i:0.8769 label_j:0.6799 score:1.5568\n",
      "-------------------------------\n",
      "label_i:0.8563 label_j:0.6674 score:1.5236\n",
      "-------------------------------\n",
      "iter_num: 2250, step:0.17% , loss: 1.3380,  label_i:0.8789 label_j:0.6799 score:1.5589\n",
      "iter_num: 2300, step:0.17% , loss: 1.2793,  label_i:0.8818 label_j:0.6799 score:1.5618\n",
      "iter_num: 2350, step:0.18% , loss: 1.2274,  label_i:0.8841 label_j:0.6799 score:1.5641\n",
      "iter_num: 2400, step:0.18% , loss: 1.1810,  label_i:0.8865 label_j:0.6799 score:1.5664\n",
      "-------------------------------\n",
      "label_i:0.8252 label_j:0.6553 score:1.4805\n",
      "-------------------------------\n",
      "iter_num: 2450, step:0.18% , loss: 1.1370,  label_i:0.8886 label_j:0.6799 score:1.5685\n",
      "iter_num: 2500, step:0.19% , loss: 1.0964,  label_i:0.8902 label_j:0.6799 score:1.5702\n",
      "iter_num: 2550, step:0.19% , loss: 1.0593,  label_i:0.8922 label_j:0.6799 score:1.5721\n",
      "iter_num: 2600, step:0.19% , loss: 1.0251,  label_i:0.8940 label_j:0.6799 score:1.5739\n",
      "-------------------------------\n",
      "label_i:0.8233 label_j:0.6447 score:1.4681\n",
      "-------------------------------\n",
      "iter_num: 2650, step:0.20% , loss: 0.9924,  label_i:0.8958 label_j:0.6799 score:1.5757\n",
      "iter_num: 2700, step:0.20% , loss: 1.3309,  label_i:0.8817 label_j:0.7257 score:1.6074\n",
      "iter_num: 2750, step:0.21% , loss: 1.2961,  label_i:0.8979 label_j:0.7004 score:1.5982\n",
      "iter_num: 2800, step:0.21% , loss: 1.2064,  label_i:0.9048 label_j:0.7299 score:1.6347\n",
      "-------------------------------\n",
      "label_i:0.8738 label_j:0.7077 score:1.5815\n",
      "-------------------------------\n",
      "score: 1.5460 -----> 1.5815\n",
      "保存模型\n",
      "iter_num: 2850, step:0.21% , loss: 1.1730,  label_i:0.9064 label_j:0.7340 score:1.6404\n",
      "iter_num: 2900, step:0.22% , loss: 1.1409,  label_i:0.9080 label_j:0.7339 score:1.6419\n",
      "iter_num: 2950, step:0.22% , loss: 1.1119,  label_i:0.9120 label_j:0.7411 score:1.6531\n",
      "iter_num: 3000, step:0.22% , loss: 1.1092,  label_i:0.9109 label_j:0.7386 score:1.6495\n",
      "-------------------------------\n",
      "label_i:0.8667 label_j:0.7013 score:1.5681\n",
      "-------------------------------\n",
      "iter_num: 3050, step:0.23% , loss: 1.0990,  label_i:0.9116 label_j:0.7417 score:1.6533\n",
      "iter_num: 3100, step:0.23% , loss: 1.0958,  label_i:0.9100 label_j:0.7449 score:1.6549\n",
      "iter_num: 3150, step:0.23% , loss: 1.0922,  label_i:0.9091 label_j:0.7464 score:1.6555\n",
      "iter_num: 3200, step:0.24% , loss: 1.0899,  label_i:0.9090 label_j:0.7489 score:1.6579\n",
      "-------------------------------\n",
      "label_i:0.8789 label_j:0.7275 score:1.6064\n",
      "-------------------------------\n",
      "score: 1.5815 -----> 1.6064\n",
      "保存模型\n",
      "iter_num: 3250, step:0.24% , loss: 1.0814,  label_i:0.9102 label_j:0.7512 score:1.6615\n",
      "iter_num: 3300, step:0.25% , loss: 1.0712,  label_i:0.9113 label_j:0.7538 score:1.6651\n",
      "iter_num: 3350, step:0.25% , loss: 1.0659,  label_i:0.9122 label_j:0.7566 score:1.6688\n",
      "iter_num: 3400, step:0.25% , loss: 1.0636,  label_i:0.9122 label_j:0.7582 score:1.6703\n",
      "-------------------------------\n",
      "label_i:0.8619 label_j:0.7113 score:1.5732\n",
      "-------------------------------\n",
      "iter_num: 3450, step:0.26% , loss: 1.0488,  label_i:0.9136 label_j:0.7617 score:1.6753\n",
      "iter_num: 3500, step:0.26% , loss: 1.0185,  label_i:0.9143 label_j:0.7628 score:1.6771\n",
      "iter_num: 3550, step:0.26% , loss: 0.9717,  label_i:0.9159 label_j:0.7628 score:1.6787\n",
      "iter_num: 3600, step:0.27% , loss: 0.9282,  label_i:0.9178 label_j:0.7628 score:1.6806\n",
      "-------------------------------\n",
      "label_i:0.8520 label_j:0.7247 score:1.5767\n",
      "-------------------------------\n",
      "iter_num: 3650, step:0.27% , loss: 0.8879,  label_i:0.9193 label_j:0.7628 score:1.6821\n",
      "iter_num: 3700, step:0.28% , loss: 0.8535,  label_i:0.9206 label_j:0.7628 score:1.6835\n",
      "iter_num: 3750, step:0.28% , loss: 0.8201,  label_i:0.9225 label_j:0.7628 score:1.6853\n",
      "iter_num: 3800, step:0.28% , loss: 0.7892,  label_i:0.9240 label_j:0.7628 score:1.6869\n",
      "-------------------------------\n",
      "label_i:0.8444 label_j:0.7084 score:1.5528\n",
      "-------------------------------\n",
      "iter_num: 3850, step:0.29% , loss: 0.7606,  label_i:0.9253 label_j:0.7628 score:1.6882\n",
      "iter_num: 3900, step:0.29% , loss: 0.7354,  label_i:0.9266 label_j:0.7628 score:1.6894\n",
      "iter_num: 3950, step:0.29% , loss: 0.7131,  label_i:0.9272 label_j:0.7628 score:1.6900\n",
      "iter_num: 4000, step:0.30% , loss: 0.6902,  label_i:0.9284 label_j:0.7628 score:1.6912\n",
      "-------------------------------\n",
      "label_i:0.8217 label_j:0.6914 score:1.5132\n",
      "-------------------------------\n",
      "iter_num: 4050, step:0.30% , loss: 1.0267,  label_i:0.9180 label_j:0.7662 score:1.6842\n",
      "iter_num: 4100, step:0.31% , loss: 0.9608,  label_i:0.9244 label_j:0.7735 score:1.6979\n",
      "iter_num: 4150, step:0.31% , loss: 0.8904,  label_i:0.9279 label_j:0.7904 score:1.7183\n",
      "iter_num: 4200, step:0.31% , loss: 0.8716,  label_i:0.9332 label_j:0.7945 score:1.7277\n",
      "-------------------------------\n",
      "label_i:0.8528 label_j:0.7219 score:1.5747\n",
      "-------------------------------\n",
      "iter_num: 4250, step:0.32% , loss: 0.8501,  label_i:0.9356 label_j:0.7968 score:1.7324\n",
      "iter_num: 4300, step:0.32% , loss: 0.8268,  label_i:0.9378 label_j:0.8010 score:1.7388\n",
      "iter_num: 4350, step:0.32% , loss: 0.8276,  label_i:0.9376 label_j:0.8024 score:1.7400\n",
      "iter_num: 4400, step:0.33% , loss: 0.8232,  label_i:0.9375 label_j:0.8031 score:1.7406\n",
      "-------------------------------\n",
      "label_i:0.8745 label_j:0.7565 score:1.6310\n",
      "-------------------------------\n",
      "score: 1.6064 -----> 1.6310\n",
      "保存模型\n",
      "iter_num: 4450, step:0.33% , loss: 0.8179,  label_i:0.9357 label_j:0.8037 score:1.7394\n",
      "iter_num: 4500, step:0.34% , loss: 0.8216,  label_i:0.9344 label_j:0.8053 score:1.7397\n",
      "iter_num: 4550, step:0.34% , loss: 0.8189,  label_i:0.9357 label_j:0.8047 score:1.7404\n",
      "iter_num: 4600, step:0.34% , loss: 0.8098,  label_i:0.9365 label_j:0.8082 score:1.7446\n",
      "-------------------------------\n",
      "label_i:0.8633 label_j:0.7417 score:1.6050\n",
      "-------------------------------\n",
      "iter_num: 4650, step:0.35% , loss: 0.8026,  label_i:0.9375 label_j:0.8102 score:1.7477\n",
      "iter_num: 4700, step:0.35% , loss: 0.8029,  label_i:0.9370 label_j:0.8113 score:1.7483\n",
      "iter_num: 4750, step:0.35% , loss: 0.8007,  label_i:0.9364 label_j:0.8120 score:1.7484\n",
      "iter_num: 4800, step:0.36% , loss: 0.7894,  label_i:0.9375 label_j:0.8148 score:1.7523\n",
      "-------------------------------\n",
      "label_i:0.8749 label_j:0.7615 score:1.6364\n",
      "-------------------------------\n",
      "score: 1.6310 -----> 1.6364\n",
      "保存模型\n",
      "iter_num: 4850, step:0.36% , loss: 0.7635,  label_i:0.9378 label_j:0.8152 score:1.7530\n",
      "iter_num: 4900, step:0.37% , loss: 0.7283,  label_i:0.9388 label_j:0.8152 score:1.7541\n",
      "iter_num: 4950, step:0.37% , loss: 0.6957,  label_i:0.9398 label_j:0.8152 score:1.7551\n",
      "iter_num: 5000, step:0.37% , loss: 0.6661,  label_i:0.9410 label_j:0.8152 score:1.7562\n",
      "-------------------------------\n",
      "label_i:0.8541 label_j:0.7353 score:1.5894\n",
      "-------------------------------\n",
      "iter_num: 5050, step:0.38% , loss: 0.6408,  label_i:0.9416 label_j:0.8152 score:1.7568\n",
      "iter_num: 5100, step:0.38% , loss: 0.6148,  label_i:0.9432 label_j:0.8152 score:1.7584\n",
      "iter_num: 5150, step:0.38% , loss: 0.5916,  label_i:0.9443 label_j:0.8152 score:1.7595\n",
      "iter_num: 5200, step:0.39% , loss: 0.5699,  label_i:0.9454 label_j:0.8152 score:1.7606\n",
      "-------------------------------\n",
      "label_i:0.8505 label_j:0.7297 score:1.5801\n",
      "-------------------------------\n",
      "iter_num: 5250, step:0.39% , loss: 0.5511,  label_i:0.9465 label_j:0.8152 score:1.7617\n",
      "iter_num: 5300, step:0.40% , loss: 0.5334,  label_i:0.9473 label_j:0.8152 score:1.7626\n",
      "iter_num: 5350, step:0.40% , loss: 0.5163,  label_i:0.9482 label_j:0.8152 score:1.7635\n",
      "iter_num: 5400, step:0.40% , loss: 0.7744,  label_i:0.9334 label_j:0.8229 score:1.7564\n",
      "-------------------------------\n",
      "label_i:0.8710 label_j:0.7473 score:1.6184\n",
      "-------------------------------\n",
      "iter_num: 5450, step:0.41% , loss: 0.7072,  label_i:0.9468 label_j:0.8343 score:1.7811\n",
      "iter_num: 5500, step:0.41% , loss: 0.6653,  label_i:0.9538 label_j:0.8410 score:1.7948\n",
      "iter_num: 5550, step:0.41% , loss: 0.6467,  label_i:0.9587 label_j:0.8458 score:1.8044\n",
      "iter_num: 5600, step:0.42% , loss: 0.6389,  label_i:0.9572 label_j:0.8477 score:1.8049\n",
      "-------------------------------\n",
      "label_i:0.8787 label_j:0.7608 score:1.6395\n",
      "-------------------------------\n",
      "score: 1.6364 -----> 1.6395\n",
      "保存模型\n",
      "iter_num: 5650, step:0.42% , loss: 0.6332,  label_i:0.9577 label_j:0.8453 score:1.8030\n",
      "iter_num: 5700, step:0.43% , loss: 0.6502,  label_i:0.9542 label_j:0.8424 score:1.7967\n",
      "iter_num: 5750, step:0.43% , loss: 0.6446,  label_i:0.9546 label_j:0.8431 score:1.7977\n",
      "iter_num: 5800, step:0.43% , loss: 0.6418,  label_i:0.9542 label_j:0.8443 score:1.7986\n",
      "-------------------------------\n",
      "label_i:0.8728 label_j:0.7580 score:1.6307\n",
      "-------------------------------\n",
      "iter_num: 5850, step:0.44% , loss: 0.6433,  label_i:0.9543 label_j:0.8457 score:1.8000\n",
      "iter_num: 5900, step:0.44% , loss: 0.6377,  label_i:0.9557 label_j:0.8477 score:1.8034\n",
      "iter_num: 5950, step:0.44% , loss: 0.6361,  label_i:0.9553 label_j:0.8474 score:1.8027\n",
      "iter_num: 6000, step:0.45% , loss: 0.6318,  label_i:0.9556 label_j:0.8490 score:1.8045\n",
      "-------------------------------\n",
      "label_i:0.8505 label_j:0.7389 score:1.5894\n",
      "-------------------------------\n",
      "iter_num: 6050, step:0.45% , loss: 0.6318,  label_i:0.9552 label_j:0.8493 score:1.8045\n",
      "iter_num: 6100, step:0.45% , loss: 0.6254,  label_i:0.9552 label_j:0.8512 score:1.8064\n",
      "iter_num: 6150, step:0.46% , loss: 0.6153,  label_i:0.9561 label_j:0.8543 score:1.8104\n",
      "iter_num: 6200, step:0.46% , loss: 0.5918,  label_i:0.9563 label_j:0.8542 score:1.8105\n",
      "-------------------------------\n",
      "label_i:0.8628 label_j:0.7438 score:1.6066\n",
      "-------------------------------\n",
      "iter_num: 6250, step:0.47% , loss: 0.5637,  label_i:0.9575 label_j:0.8542 score:1.8117\n",
      "iter_num: 6300, step:0.47% , loss: 0.5393,  label_i:0.9581 label_j:0.8542 score:1.8123\n",
      "iter_num: 6350, step:0.47% , loss: 0.5163,  label_i:0.9589 label_j:0.8542 score:1.8131\n",
      "iter_num: 6400, step:0.48% , loss: 0.4960,  label_i:0.9593 label_j:0.8542 score:1.8135\n",
      "-------------------------------\n",
      "label_i:0.8602 label_j:0.7374 score:1.5976\n",
      "-------------------------------\n",
      "iter_num: 6450, step:0.48% , loss: 0.4753,  label_i:0.9604 label_j:0.8542 score:1.8146\n",
      "iter_num: 6500, step:0.48% , loss: 0.4565,  label_i:0.9615 label_j:0.8542 score:1.8157\n",
      "iter_num: 6550, step:0.49% , loss: 0.4405,  label_i:0.9620 label_j:0.8542 score:1.8162\n",
      "iter_num: 6600, step:0.49% , loss: 0.4266,  label_i:0.9624 label_j:0.8542 score:1.8166\n",
      "-------------------------------\n",
      "label_i:0.8541 label_j:0.7396 score:1.5936\n",
      "-------------------------------\n",
      "iter_num: 6650, step:0.50% , loss: 0.4125,  label_i:0.9630 label_j:0.8542 score:1.8172\n",
      "iter_num: 6700, step:0.50% , loss: 0.3993,  label_i:0.9636 label_j:0.8542 score:1.8178\n",
      "iter_num: 6750, step:0.50% , loss: 0.5705,  label_i:0.9697 label_j:0.8708 score:1.8405\n",
      "iter_num: 6800, step:0.51% , loss: 0.5479,  label_i:0.9656 label_j:0.8711 score:1.8367\n",
      "-------------------------------\n",
      "label_i:0.8609 label_j:0.7565 score:1.6174\n",
      "-------------------------------\n",
      "iter_num: 6850, step:0.51% , loss: 0.5232,  label_i:0.9687 label_j:0.8793 score:1.8480\n",
      "iter_num: 6900, step:0.51% , loss: 0.5131,  label_i:0.9707 label_j:0.8801 score:1.8508\n",
      "iter_num: 6950, step:0.52% , loss: 0.5119,  label_i:0.9705 label_j:0.8770 score:1.8475\n",
      "iter_num: 7000, step:0.52% , loss: 0.5020,  label_i:0.9709 label_j:0.8778 score:1.8486\n",
      "-------------------------------\n",
      "label_i:0.8697 label_j:0.7615 score:1.6312\n",
      "-------------------------------\n",
      "iter_num: 7050, step:0.53% , loss: 0.5027,  label_i:0.9706 label_j:0.8784 score:1.8491\n",
      "iter_num: 7100, step:0.53% , loss: 0.5000,  label_i:0.9703 label_j:0.8772 score:1.8476\n",
      "iter_num: 7150, step:0.53% , loss: 0.4941,  label_i:0.9702 label_j:0.8794 score:1.8496\n",
      "iter_num: 7200, step:0.54% , loss: 0.4972,  label_i:0.9705 label_j:0.8797 score:1.8501\n",
      "-------------------------------\n",
      "label_i:0.8714 label_j:0.7693 score:1.6407\n",
      "-------------------------------\n",
      "score: 1.6395 -----> 1.6407\n",
      "保存模型\n",
      "iter_num: 7250, step:0.54% , loss: 0.4895,  label_i:0.9717 label_j:0.8800 score:1.8517\n",
      "iter_num: 7300, step:0.54% , loss: 0.4863,  label_i:0.9719 label_j:0.8817 score:1.8536\n",
      "iter_num: 7350, step:0.55% , loss: 0.4840,  label_i:0.9721 label_j:0.8836 score:1.8557\n",
      "iter_num: 7400, step:0.55% , loss: 0.4806,  label_i:0.9718 label_j:0.8845 score:1.8564\n",
      "-------------------------------\n",
      "label_i:0.8788 label_j:0.7672 score:1.6460\n",
      "-------------------------------\n",
      "score: 1.6407 -----> 1.6460\n",
      "保存模型\n",
      "iter_num: 7450, step:0.56% , loss: 0.4782,  label_i:0.9712 label_j:0.8853 score:1.8565\n",
      "iter_num: 7500, step:0.56% , loss: 0.4711,  label_i:0.9719 label_j:0.8872 score:1.8591\n",
      "iter_num: 7550, step:0.56% , loss: 0.4487,  label_i:0.9721 label_j:0.8872 score:1.8592\n",
      "iter_num: 7600, step:0.57% , loss: 0.4278,  label_i:0.9725 label_j:0.8872 score:1.8597\n",
      "-------------------------------\n",
      "label_i:0.8755 label_j:0.7771 score:1.6526\n",
      "-------------------------------\n",
      "score: 1.6460 -----> 1.6526\n",
      "保存模型\n",
      "iter_num: 7650, step:0.57% , loss: 0.4085,  label_i:0.9731 label_j:0.8872 score:1.8603\n",
      "iter_num: 7700, step:0.57% , loss: 0.3921,  label_i:0.9731 label_j:0.8872 score:1.8603\n",
      "iter_num: 7750, step:0.58% , loss: 0.3767,  label_i:0.9734 label_j:0.8872 score:1.8606\n",
      "iter_num: 7800, step:0.58% , loss: 0.3616,  label_i:0.9738 label_j:0.8872 score:1.8609\n",
      "-------------------------------\n",
      "label_i:0.8421 label_j:0.7424 score:1.5845\n",
      "-------------------------------\n",
      "iter_num: 7850, step:0.59% , loss: 0.3481,  label_i:0.9742 label_j:0.8872 score:1.8614\n",
      "iter_num: 7900, step:0.59% , loss: 0.3354,  label_i:0.9748 label_j:0.8872 score:1.8620\n",
      "iter_num: 7950, step:0.59% , loss: 0.3248,  label_i:0.9751 label_j:0.8872 score:1.8623\n",
      "iter_num: 8000, step:0.60% , loss: 0.3143,  label_i:0.9754 label_j:0.8872 score:1.8625\n",
      "-------------------------------\n",
      "label_i:0.8482 label_j:0.7459 score:1.5942\n",
      "-------------------------------\n",
      "iter_num: 8050, step:0.60% , loss: 0.2916,  label_i:1.0000 label_j:0.9219 score:1.9219\n",
      "iter_num: 8100, step:0.60% , loss: 0.4371,  label_i:0.9795 label_j:0.8924 score:1.8719\n",
      "iter_num: 8150, step:0.61% , loss: 0.4103,  label_i:0.9800 label_j:0.8996 score:1.8796\n",
      "iter_num: 8200, step:0.61% , loss: 0.4082,  label_i:0.9797 label_j:0.9002 score:1.8799\n",
      "-------------------------------\n",
      "label_i:0.8794 label_j:0.7700 score:1.6494\n",
      "-------------------------------\n",
      "iter_num: 8250, step:0.62% , loss: 0.4029,  label_i:0.9818 label_j:0.9004 score:1.8823\n",
      "iter_num: 8300, step:0.62% , loss: 0.3920,  label_i:0.9824 label_j:0.9043 score:1.8866\n",
      "iter_num: 8350, step:0.62% , loss: 0.3927,  label_i:0.9819 label_j:0.9021 score:1.8840\n",
      "iter_num: 8400, step:0.63% , loss: 0.3977,  label_i:0.9805 label_j:0.9032 score:1.8837\n",
      "-------------------------------\n",
      "label_i:0.8809 label_j:0.7643 score:1.6452\n",
      "-------------------------------\n",
      "iter_num: 8450, step:0.63% , loss: 0.3919,  label_i:0.9810 label_j:0.9050 score:1.8860\n",
      "iter_num: 8500, step:0.63% , loss: 0.3913,  label_i:0.9804 label_j:0.9067 score:1.8870\n",
      "iter_num: 8550, step:0.64% , loss: 0.3929,  label_i:0.9799 label_j:0.9061 score:1.8860\n",
      "iter_num: 8600, step:0.64% , loss: 0.3870,  label_i:0.9806 label_j:0.9069 score:1.8875\n",
      "-------------------------------\n",
      "label_i:0.8741 label_j:0.7665 score:1.6405\n",
      "-------------------------------\n",
      "iter_num: 8650, step:0.65% , loss: 0.3819,  label_i:0.9808 label_j:0.9085 score:1.8894\n",
      "iter_num: 8700, step:0.65% , loss: 0.3816,  label_i:0.9807 label_j:0.9099 score:1.8906\n",
      "iter_num: 8750, step:0.65% , loss: 0.3793,  label_i:0.9812 label_j:0.9105 score:1.8918\n",
      "iter_num: 8800, step:0.66% , loss: 0.3748,  label_i:0.9810 label_j:0.9116 score:1.8926\n",
      "-------------------------------\n",
      "label_i:0.8707 label_j:0.7665 score:1.6371\n",
      "-------------------------------\n",
      "iter_num: 8850, step:0.66% , loss: 0.3680,  label_i:0.9809 label_j:0.9125 score:1.8934\n",
      "iter_num: 8900, step:0.66% , loss: 0.3504,  label_i:0.9808 label_j:0.9125 score:1.8933\n",
      "iter_num: 8950, step:0.67% , loss: 0.3335,  label_i:0.9813 label_j:0.9125 score:1.8938\n",
      "iter_num: 9000, step:0.67% , loss: 0.3187,  label_i:0.9814 label_j:0.9125 score:1.8939\n",
      "-------------------------------\n",
      "label_i:0.8673 label_j:0.7650 score:1.6323\n",
      "-------------------------------\n",
      "iter_num: 9050, step:0.67% , loss: 0.3063,  label_i:0.9814 label_j:0.9125 score:1.8939\n",
      "iter_num: 9100, step:0.68% , loss: 0.2935,  label_i:0.9818 label_j:0.9125 score:1.8943\n",
      "iter_num: 9150, step:0.68% , loss: 0.2815,  label_i:0.9821 label_j:0.9125 score:1.8946\n",
      "iter_num: 9200, step:0.69% , loss: 0.2707,  label_i:0.9826 label_j:0.9125 score:1.8951\n",
      "-------------------------------\n",
      "label_i:0.8500 label_j:0.7594 score:1.6093\n",
      "-------------------------------\n",
      "iter_num: 9250, step:0.69% , loss: 0.2612,  label_i:0.9828 label_j:0.9125 score:1.8953\n",
      "iter_num: 9300, step:0.69% , loss: 0.2524,  label_i:0.9828 label_j:0.9125 score:1.8953\n",
      "iter_num: 9350, step:0.70% , loss: 0.2440,  label_i:0.9833 label_j:0.9125 score:1.8958\n",
      "iter_num: 9400, step:0.70% , loss: 0.3071,  label_i:0.9852 label_j:0.9327 score:1.9178\n",
      "-------------------------------\n",
      "label_i:0.8524 label_j:0.7573 score:1.6097\n",
      "-------------------------------\n",
      "iter_num: 9450, step:0.70% , loss: 0.3395,  label_i:0.9799 label_j:0.9246 score:1.9045\n",
      "iter_num: 9500, step:0.71% , loss: 0.3238,  label_i:0.9807 label_j:0.9259 score:1.9066\n",
      "iter_num: 9550, step:0.71% , loss: 0.3164,  label_i:0.9842 label_j:0.9264 score:1.9106\n",
      "iter_num: 9600, step:0.72% , loss: 0.3044,  label_i:0.9857 label_j:0.9287 score:1.9144\n",
      "-------------------------------\n",
      "label_i:0.8793 label_j:0.7841 score:1.6634\n",
      "-------------------------------\n",
      "score: 1.6526 -----> 1.6634\n",
      "保存模型\n",
      "iter_num: 9650, step:0.72% , loss: 0.3061,  label_i:0.9850 label_j:0.9266 score:1.9116\n",
      "iter_num: 9700, step:0.72% , loss: 0.3009,  label_i:0.9861 label_j:0.9271 score:1.9132\n",
      "iter_num: 9750, step:0.73% , loss: 0.3052,  label_i:0.9860 label_j:0.9275 score:1.9135\n",
      "iter_num: 9800, step:0.73% , loss: 0.3017,  label_i:0.9868 label_j:0.9283 score:1.9150\n",
      "-------------------------------\n",
      "label_i:0.8768 label_j:0.7757 score:1.6525\n",
      "-------------------------------\n",
      "iter_num: 9850, step:0.73% , loss: 0.3028,  label_i:0.9869 label_j:0.9285 score:1.9154\n",
      "iter_num: 9900, step:0.74% , loss: 0.3040,  label_i:0.9867 label_j:0.9284 score:1.9151\n",
      "iter_num: 9950, step:0.74% , loss: 0.2991,  label_i:0.9871 label_j:0.9297 score:1.9169\n",
      "iter_num: 10000, step:0.75% , loss: 0.2970,  label_i:0.9872 label_j:0.9301 score:1.9173\n",
      "-------------------------------\n",
      "label_i:0.8842 label_j:0.7785 score:1.6627\n",
      "-------------------------------\n",
      "iter_num: 10050, step:0.75% , loss: 0.2990,  label_i:0.9872 label_j:0.9302 score:1.9174\n",
      "iter_num: 10100, step:0.75% , loss: 0.2968,  label_i:0.9873 label_j:0.9303 score:1.9176\n",
      "iter_num: 10150, step:0.76% , loss: 0.2927,  label_i:0.9873 label_j:0.9309 score:1.9183\n",
      "iter_num: 10200, step:0.76% , loss: 0.2854,  label_i:0.9874 label_j:0.9315 score:1.9189\n",
      "-------------------------------\n",
      "label_i:0.8783 label_j:0.7813 score:1.6596\n",
      "-------------------------------\n",
      "iter_num: 10250, step:0.76% , loss: 0.2720,  label_i:0.9872 label_j:0.9315 score:1.9187\n",
      "iter_num: 10300, step:0.77% , loss: 0.2596,  label_i:0.9872 label_j:0.9315 score:1.9186\n",
      "iter_num: 10350, step:0.77% , loss: 0.2476,  label_i:0.9875 label_j:0.9315 score:1.9189\n",
      "iter_num: 10400, step:0.78% , loss: 0.2383,  label_i:0.9873 label_j:0.9315 score:1.9188\n",
      "-------------------------------\n",
      "label_i:0.8601 label_j:0.7665 score:1.6265\n",
      "-------------------------------\n",
      "iter_num: 10450, step:0.78% , loss: 0.2289,  label_i:0.9874 label_j:0.9315 score:1.9189\n",
      "iter_num: 10500, step:0.78% , loss: 0.2197,  label_i:0.9877 label_j:0.9315 score:1.9192\n",
      "iter_num: 10550, step:0.79% , loss: 0.2113,  label_i:0.9880 label_j:0.9315 score:1.9195\n",
      "iter_num: 10600, step:0.79% , loss: 0.2043,  label_i:0.9881 label_j:0.9315 score:1.9196\n",
      "-------------------------------\n",
      "label_i:0.8615 label_j:0.7643 score:1.6258\n",
      "-------------------------------\n",
      "iter_num: 10650, step:0.79% , loss: 0.1983,  label_i:0.9881 label_j:0.9315 score:1.9196\n",
      "iter_num: 10700, step:0.80% , loss: 0.1918,  label_i:0.9882 label_j:0.9315 score:1.9197\n",
      "iter_num: 10750, step:0.80% , loss: 0.2094,  label_i:0.9966 label_j:0.9517 score:1.9483\n",
      "iter_num: 10800, step:0.81% , loss: 0.2549,  label_i:0.9933 label_j:0.9418 score:1.9351\n",
      "-------------------------------\n",
      "label_i:0.8799 label_j:0.7792 score:1.6591\n",
      "-------------------------------\n",
      "iter_num: 10850, step:0.81% , loss: 0.2432,  label_i:0.9926 label_j:0.9426 score:1.9353\n",
      "iter_num: 10900, step:0.81% , loss: 0.2476,  label_i:0.9926 label_j:0.9411 score:1.9337\n",
      "iter_num: 10950, step:0.82% , loss: 0.2435,  label_i:0.9923 label_j:0.9417 score:1.9340\n",
      "iter_num: 11000, step:0.82% , loss: 0.2361,  label_i:0.9926 label_j:0.9439 score:1.9365\n",
      "-------------------------------\n",
      "label_i:0.8880 label_j:0.7962 score:1.6841\n",
      "-------------------------------\n",
      "score: 1.6634 -----> 1.6841\n",
      "保存模型\n",
      "iter_num: 11050, step:0.82% , loss: 0.2370,  label_i:0.9926 label_j:0.9437 score:1.9363\n",
      "iter_num: 11100, step:0.83% , loss: 0.2385,  label_i:0.9916 label_j:0.9449 score:1.9365\n",
      "iter_num: 11150, step:0.83% , loss: 0.2383,  label_i:0.9915 label_j:0.9458 score:1.9373\n",
      "iter_num: 11200, step:0.84% , loss: 0.2409,  label_i:0.9911 label_j:0.9465 score:1.9376\n",
      "-------------------------------\n",
      "label_i:0.8841 label_j:0.7870 score:1.6711\n",
      "-------------------------------\n",
      "iter_num: 11250, step:0.84% , loss: 0.2391,  label_i:0.9916 label_j:0.9467 score:1.9383\n",
      "iter_num: 11300, step:0.84% , loss: 0.2348,  label_i:0.9919 label_j:0.9470 score:1.9389\n",
      "iter_num: 11350, step:0.85% , loss: 0.2332,  label_i:0.9918 label_j:0.9471 score:1.9389\n",
      "iter_num: 11400, step:0.85% , loss: 0.2330,  label_i:0.9916 label_j:0.9477 score:1.9393\n",
      "-------------------------------\n",
      "label_i:0.8809 label_j:0.7948 score:1.6757\n",
      "-------------------------------\n",
      "iter_num: 11450, step:0.85% , loss: 0.2308,  label_i:0.9918 label_j:0.9491 score:1.9409\n",
      "iter_num: 11500, step:0.86% , loss: 0.2278,  label_i:0.9921 label_j:0.9496 score:1.9416\n",
      "iter_num: 11550, step:0.86% , loss: 0.2203,  label_i:0.9922 label_j:0.9498 score:1.9420\n",
      "iter_num: 11600, step:0.87% , loss: 0.2099,  label_i:0.9921 label_j:0.9498 score:1.9419\n",
      "-------------------------------\n",
      "label_i:0.8751 label_j:0.7834 score:1.6586\n",
      "-------------------------------\n",
      "iter_num: 11650, step:0.87% , loss: 0.2001,  label_i:0.9922 label_j:0.9498 score:1.9420\n",
      "iter_num: 11700, step:0.87% , loss: 0.1910,  label_i:0.9924 label_j:0.9498 score:1.9422\n",
      "iter_num: 11750, step:0.88% , loss: 0.1838,  label_i:0.9922 label_j:0.9498 score:1.9420\n",
      "iter_num: 11800, step:0.88% , loss: 0.1763,  label_i:0.9924 label_j:0.9498 score:1.9422\n",
      "-------------------------------\n",
      "label_i:0.8720 label_j:0.7749 score:1.6469\n",
      "-------------------------------\n",
      "iter_num: 11850, step:0.88% , loss: 0.1691,  label_i:0.9927 label_j:0.9498 score:1.9425\n",
      "iter_num: 11900, step:0.89% , loss: 0.1635,  label_i:0.9926 label_j:0.9498 score:1.9424\n",
      "iter_num: 11950, step:0.89% , loss: 0.1583,  label_i:0.9926 label_j:0.9498 score:1.9424\n",
      "iter_num: 12000, step:0.89% , loss: 0.1533,  label_i:0.9926 label_j:0.9498 score:1.9424\n",
      "-------------------------------\n",
      "label_i:0.8641 label_j:0.7749 score:1.6390\n",
      "-------------------------------\n",
      "iter_num: 12050, step:0.90% , loss: 0.1483,  label_i:0.9927 label_j:0.9498 score:1.9425\n",
      "iter_num: 12100, step:0.90% , loss: 0.1882,  label_i:0.9938 label_j:0.9657 score:1.9595\n",
      "iter_num: 12150, step:0.91% , loss: 0.1936,  label_i:0.9949 label_j:0.9599 score:1.9548\n",
      "iter_num: 12200, step:0.91% , loss: 0.1958,  label_i:0.9960 label_j:0.9537 score:1.9497\n",
      "-------------------------------\n",
      "label_i:0.8732 label_j:0.7834 score:1.6566\n",
      "-------------------------------\n",
      "iter_num: 12250, step:0.91% , loss: 0.2058,  label_i:0.9945 label_j:0.9541 score:1.9486\n",
      "iter_num: 12300, step:0.92% , loss: 0.2003,  label_i:0.9951 label_j:0.9556 score:1.9507\n",
      "iter_num: 12350, step:0.92% , loss: 0.1957,  label_i:0.9953 label_j:0.9566 score:1.9519\n",
      "iter_num: 12400, step:0.92% , loss: 0.1958,  label_i:0.9943 label_j:0.9571 score:1.9514\n",
      "-------------------------------\n",
      "label_i:0.8827 label_j:0.7898 score:1.6725\n",
      "-------------------------------\n",
      "iter_num: 12450, step:0.93% , loss: 0.1941,  label_i:0.9947 label_j:0.9570 score:1.9517\n",
      "iter_num: 12500, step:0.93% , loss: 0.1946,  label_i:0.9944 label_j:0.9574 score:1.9518\n",
      "iter_num: 12550, step:0.94% , loss: 0.1966,  label_i:0.9945 label_j:0.9573 score:1.9518\n",
      "iter_num: 12600, step:0.94% , loss: 0.1944,  label_i:0.9949 label_j:0.9574 score:1.9523\n",
      "-------------------------------\n",
      "label_i:0.8841 label_j:0.7905 score:1.6746\n",
      "-------------------------------\n",
      "iter_num: 12650, step:0.94% , loss: 0.1918,  label_i:0.9954 label_j:0.9578 score:1.9532\n",
      "iter_num: 12700, step:0.95% , loss: 0.1894,  label_i:0.9952 label_j:0.9581 score:1.9533\n",
      "iter_num: 12750, step:0.95% , loss: 0.1906,  label_i:0.9947 label_j:0.9580 score:1.9526\n",
      "iter_num: 12800, step:0.95% , loss: 0.1887,  label_i:0.9949 label_j:0.9584 score:1.9532\n",
      "-------------------------------\n",
      "label_i:0.8812 label_j:0.7919 score:1.6732\n",
      "-------------------------------\n",
      "iter_num: 12850, step:0.96% , loss: 0.1858,  label_i:0.9951 label_j:0.9590 score:1.9541\n",
      "iter_num: 12900, step:0.96% , loss: 0.1784,  label_i:0.9951 label_j:0.9590 score:1.9541\n",
      "iter_num: 12950, step:0.97% , loss: 0.1700,  label_i:0.9951 label_j:0.9590 score:1.9541\n",
      "iter_num: 13000, step:0.97% , loss: 0.1626,  label_i:0.9948 label_j:0.9590 score:1.9538\n",
      "-------------------------------\n",
      "label_i:0.8758 label_j:0.7849 score:1.6607\n",
      "-------------------------------\n",
      "iter_num: 13050, step:0.97% , loss: 0.1562,  label_i:0.9947 label_j:0.9590 score:1.9537\n",
      "iter_num: 13100, step:0.98% , loss: 0.1503,  label_i:0.9946 label_j:0.9590 score:1.9536\n",
      "iter_num: 13150, step:0.98% , loss: 0.1443,  label_i:0.9946 label_j:0.9590 score:1.9536\n",
      "iter_num: 13200, step:0.98% , loss: 0.1388,  label_i:0.9947 label_j:0.9590 score:1.9537\n",
      "-------------------------------\n",
      "label_i:0.8667 label_j:0.7827 score:1.6495\n",
      "-------------------------------\n",
      "iter_num: 13250, step:0.99% , loss: 0.1340,  label_i:0.9946 label_j:0.9590 score:1.9536\n",
      "iter_num: 13300, step:0.99% , loss: 0.1301,  label_i:0.9945 label_j:0.9590 score:1.9535\n",
      "iter_num: 13350, step:1.00% , loss: 0.1261,  label_i:0.9945 label_j:0.9590 score:1.9535\n",
      "iter_num: 13400, step:1.00% , loss: 0.1224,  label_i:0.9944 label_j:0.9590 score:1.9534\n",
      "-------------------------------\n",
      "label_i:0.8697 label_j:0.7813 score:1.6510\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 开始训练，学习率设置较小，训练轮次尽量大一些\n",
    "epochs = 10\n",
    "best_score = train(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('最优线下成绩：', 1.684131238480608)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"最优线下成绩：\",best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 475/475 [00:49<00:00,  9.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# 加载最佳模型，提交文件\n",
    "model = torch.load('model_best.bin')\n",
    "test_pred_i, test_pred_j = prediction()\n",
    "test_submit['label_i'] = test_pred_i\n",
    "test_submit['label_j'] = test_pred_j\n",
    "test_submit.to_csv('bert_submit.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'线上测试成绩:1.65+ '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"线上测试成绩:1.65+ / \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "| base-model  | score   |\n",
    "|-------------|---------|\n",
    "| mc-bert     | 1.6507  |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
